# Handwritten Donut â€” English + Kannada OCR (Swin Transformer)

A clean, production-ready system for recognizing handwritten English and Kannada characters using deep learning.


ğŸ¤” What is this?
Think of this as a smart scanner that can read handwritten characters. You show it an image of a handwritten letter (like 'a' or 'à²…'), and it tells you what it is.
The magic: It uses AI to learn from examples, just like how you learned to read as a kid!

ğŸ¨ What makes this special?
Your original code worked, but it was like a messy kitchen where everything is in one drawer. This version is like a professional kitchen - everything has its place, and it's much faster!
The Big Changes:

Organized Structure ğŸ“

Before: Everything jumbled in one file
Now: Separate folders for data, models, and utilities
Why? Easy to find things, easy to fix bugs, easy for teams to work together


Blazing Fast âš¡

Before: Preprocessing happened every time (slow!)
Now: Preprocessing happens once when loading data
Result: 3-5x faster training!


Smart Training ğŸ§ 

Before: Just showed you loss (confusing!)
Now: Shows accuracy, character accuracy, error rates
Plus: Automatically saves best model, stops if it's not learning


Easy to Adjust âš™ï¸

Before: Had to edit code to change settings
Now: Just edit config.yaml - no code changes needed!




ğŸ“¦ What's in the box?
handwritten-recognition/
â”‚
â”œâ”€â”€ ğŸ›ï¸ config.yaml           # All your settings (like a control panel)
â”œâ”€â”€ ğŸ“‹ requirements.txt      # Software needed to run this
â”‚
â”œâ”€â”€ ğŸš‚ train.py              # Trains the AI model
â”œâ”€â”€ ğŸ”® inference.py          # Uses trained model to predict
â”‚
â”œâ”€â”€ ğŸ“‚ data/                 # Everything about loading data
â”‚   â”œâ”€â”€ tokenizer.py         # Converts characters â†” numbers
â”‚   â””â”€â”€ dataset.py           # Loads and prepares images
â”‚
â”œâ”€â”€ ğŸ¤– models/               # The AI brain
â”‚   â”œâ”€â”€ encoder.py           # Looks at images
â”‚   â”œâ”€â”€ decoder.py           # Generates text predictions
â”‚   â””â”€â”€ model.py             # Combines everything
â”‚
â””â”€â”€ ğŸ› ï¸ utils/                # Helper tools
    â”œâ”€â”€ preprocessing.py     # Cleans up images
    â”œâ”€â”€ metrics.py           # Measures how good the AI is
    â””â”€â”€ training.py          # Training helpers (saving, logging, etc.)

ğŸš€ Getting Started (5 minutes!)
Step 1: Set everything up
bash# Create the project folder
mkdir handwritten-recognition
cd handwritten-recognition

# Create subfolders
mkdir data models utils checkpoints logs outputs

# Create special files Python needs
touch data/__init__.py models/__init__.py utils/__init__.py

# Install required software
pip install -r requirements.txt
```

### Step 2: Organize your images

Your images should look like this:
```
dataset/
â””â”€â”€ training_images/
    â”œâ”€â”€ a/              â† Put all images of letter 'a' here
    â”‚   â”œâ”€â”€ img1.png
    â”‚   â”œâ”€â”€ img2.png
    â”‚   â””â”€â”€ img3.png
    â”œâ”€â”€ b/              â† Put all images of letter 'b' here
    â”‚   â”œâ”€â”€ img1.png
    â”‚   â””â”€â”€ img2.png
    â”œâ”€â”€ à²…/              â† Put all images of 'à²…' here
    â”‚   â””â”€â”€ img1.png
    â””â”€â”€ ...
The folder name = the character in the images!
Step 3: Tell it where your images are
Open config.yaml and change this line:
yamldata:
  train_path: "/path/to/your/dataset/training_images"  # â† Put your actual path here
Step 4: Start training!
bashpython train.py --config config.yaml
```

Now sit back! The AI will:
- âœ… Load your images
- âœ… Learn from them
- âœ… Save the best model automatically
- âœ… Show you how well it's learning

---

## ğŸ“Š What you'll see while training
```
Epoch 1/50
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Train: loss: 2.35 | accuracy: 23.45% | cer: 54.33%
Val:   loss: 2.12 | accuracy: 28.90% | cer: 47.66%
âœ“ New best model saved!

Epoch 2/50
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Train: loss: 1.87 | accuracy: 45.67% | cer: 32.11%
Val:   loss: 1.76 | accuracy: 52.34% | cer: 28.77%
âœ“ New best model saved!

... (getting better each time!)

Epoch 25/50
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Train: loss: 0.23 | accuracy: 94.50% | cer: 3.21%
Val:   loss: 0.31 | accuracy: 91.20% | cer: 5.43%
âœ“ New best model saved!
What these numbers mean:

Loss: Lower is better (think: how wrong it is)
Accuracy: Higher is better (% of perfect matches)
CER (Character Error Rate): Lower is better (% of mistakes)


ğŸ”® Using your trained model
Once training finishes, use it to read new images:
bash# Read one image
python inference.py \
    --checkpoint checkpoints/best_model.pth \
    --image my_handwriting.png

# Result: Prediction: a
bash# Read many images at once
python inference.py \
    --checkpoint checkpoints/best_model.pth \
    --image_dir my_images/ \
    --output results.txt

âš™ï¸ Tweaking Settings
All settings are in config.yaml. Here's what you might want to change:
Training too slow?
yamldata:
  batch_size: 16  # Process more images at once (needs more GPU memory)
Not learning well?
yamltraining:
  learning_rate: 5e-4  # Make it learn faster
  epochs: 100          # Train for longer
Running out of memory?
yamldata:
  batch_size: 4  # Process fewer images at once
Images are very clean (printed, not handwritten)?
yamldata:
  apply_manuscript_preprocessing: false  # Turn off aggressive cleaning

ğŸ› Something not working?
"Module not found" error
bash# Did you create these files?
touch data/__init__.py models/__init__.py utils/__init__.py
"CUDA out of memory"
yaml# In config.yaml, reduce batch size:
data:
  batch_size: 4
"No images found"
bash# Check your dataset path in config.yaml
# Make sure images are in folders named after their labels
ls /your/path/training_images/
Still stuck?
bash# Check the training log for clues
cat logs/training.log

ğŸ“ˆ How to know if it's working?
Good signs:

âœ… Loss going down each epoch
âœ… Accuracy going up
âœ… CER (error rate) going down
âœ… Training and validation metrics are similar

Warning signs:

âš ï¸ Loss not changing â†’ learning rate might be wrong
âš ï¸ Training accuracy high but validation low â†’ overfitting (train longer, add more data)
âš ï¸ Loss becomes "nan" â†’ learning rate too highd
